{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632a7bcb",
   "metadata": {},
   "source": [
    "### Design Summary\n",
    "\n",
    "This Python script addresses a common bottleneck in tabular machine learning workflows, **Exploratory Data Analysis (EDA)**,  particularly when datasets have many features or high-cardinality categorical variables:  \n",
    "\n",
    "**Key problems it solves:**  \n",
    "- Systematically analyzes numeric and categorical features for:  \n",
    "  - Missing values and imputation strategy  \n",
    "  - Outliers and distributional skew  \n",
    "  - Feature scaling  \n",
    "  - Cardinality and encoding decisions  \n",
    "  - Multicollinearity between features  \n",
    "- Reduces feature explosion caused by one-hot encoding of high-cardinality categorical variables  \n",
    "- Ensures target-aware preprocessing decisions to prevent information leakage  \n",
    "\n",
    "**What the script provides:**  \n",
    "- Detailed, structured summary artifacts documenting each preprocessing recommendation  \n",
    "- Transparency and reproducibility for all decisions, including:  \n",
    "  - Imputation method  \n",
    "  - Distributional transformations  \n",
    "  - Scaling  \n",
    "  - Encoding  \n",
    "  - Feature removal  \n",
    "- Guidance for constructing robust, interpretable data pipelines specifically for Linear Regression modeling  \n",
    "\n",
    "**Outcome:**  \n",
    "Users gain actionable, defensible preprocessing guidance, (as a result of automated EDA), that scales to large or complex datasets, helping them build efficient pipelines without manually inspecting every feature. \n",
    "\n",
    "Note: The architecture, workflow, and functionality of this project are entirely my own. Generative AI was used for some coding tasks, like formatting and debugging. In an effort to modernize my approach and stay current with emerging technologies, I treat AI as a collaborative partner to enhance productivity and creativity, while all key decisions, project structure, and underlying logic remain under human control.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282daa0a",
   "metadata": {},
   "source": [
    "### Core Data Libraries\n",
    "\n",
    "\n",
    "- **NumPy** : Supports numerical operations and reproducible random sampling for constructing example categorical features; used for analysis only.\n",
    "\n",
    "- **Pandas**: Provides the DataFrame structure used to inspect missingness, cardinality, and data types that drive preprocessing recommendations.\n",
    "\n",
    "#### Scikit-learn Components\n",
    "\n",
    "- **California Housing loader**: Supplies a realistic regression dataset used solely for analysis and rule evaluation.\n",
    "\n",
    "- **Train–test split utilities**: Included to conceptually enforce that recommendation-driving statistics come from training data only.\n",
    "\n",
    "- **KNN Imputer / Iterative Imputer**: Imported as reference implementations for distance-based and model-based imputation strategies; not executed. The iterative imputer is explicitly enabled due to its experimental status.\n",
    "\n",
    "#### System Utilities\n",
    "\n",
    "- **OS interface**: Used to load external configuration files (e.g., ordinal feature lists) so domain knowledge informs recommendations without being hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d918292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6602b",
   "metadata": {},
   "source": [
    "### Dataset Loading and Target Definition\n",
    "\n",
    "This section loads the base dataset and defines the regression target used for all preprocessing recommendations. The dataset is treated as read-only and is used solely for feature inspection and analysis.\n",
    "\n",
    "The California Housing dataset is loaded directly into a Pandas DataFrame to preserve column names and data types. The target variable is explicitly identified as the median house value, ensuring a clear separation between predictors and the response variable throughout the recommendation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1496c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Load dataset ----------------\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "target_col = \"MedHouseVal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437502bb",
   "metadata": {},
   "source": [
    "### Sample Categorical columns\n",
    "\n",
    "The California Housing dataset does not contain any native categorical features; therefore, categorical columns were introduced solely for demonstration purposes. When working with datasets that do not include categorical variables, this section of the script may be safely removed. If no categorical features are present, the script will still execute without error and will explicitly annotate the output files to indicate that no categorical variables were detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f0a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Example categorical columns ----------------\n",
    "np.random.seed(42)\n",
    "df['Neighborhood'] = np.random.choice([f\"N{i}\" for i in range(1, 36)] + [np.nan], size=len(df), p=[0.027]*35 + [0.055])\n",
    "df['HouseStyle'] = np.random.choice(['Ranch','Split','Colonial',np.nan], size=len(df), p=[0.3,0.3,0.3,0.1])\n",
    "df['Flag'] = np.random.choice(['Yes','No'], size=len(df), p=[0.5,0.5])\n",
    "df['Misc'] = np.random.choice(['A','B','C','D','E','F','G','H', np.nan], size=len(df), p=[0.1]*8+[0.2])\n",
    "df['Quality'] = np.random.choice(['Low','Medium','High', np.nan], size=len(df), p=[0.3,0.5,0.15,0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc81cd7",
   "metadata": {},
   "source": [
    "### Feature–Target Separation and Dataset Splitting\n",
    "\n",
    "This section separates predictor features from the target variable and defines training and test datasets for use in preprocessing recommendation logic.\n",
    "\n",
    "The target column is explicitly removed from the feature set to ensure it is never analyzed as an input feature. This enforces a clean conceptual boundary between predictors and the response variable when evaluating feature properties.\n",
    "\n",
    "The dataset is then split into training and test subsets using a fixed random state. Although no preprocessing is applied, this split establishes the principle that any statistics or heuristics used to inform preprocessing recommendations should be derived from training data only, preventing implicit data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b843d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Features / target split ----------------\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849052f9",
   "metadata": {},
   "source": [
    "### Preprocessing Configuration Thresholds\n",
    "\n",
    "This section defines global thresholds that control how preprocessing recommendations are generated. These values parameterize decision logic rather than enforcing fixed rules, making the recommendation system transparent and tunable.\n",
    "\n",
    "- **Rare category threshold**: Defines the minimum proportion a category must represent before being considered stable. Categories below this threshold may trigger consolidation, alternative encoding strategies, or drop recommendations.\n",
    "\n",
    "- **Missingness thresholds**: Establish escalation levels for handling missing data. Low missingness supports simple imputation recommendations, medium missingness suggests more robust strategies, and high missingness may justify dropping a feature.\n",
    "\n",
    "- **Dataset size threshold**: Differentiates recommendation behavior based on dataset scale, such as when one-hot encoding is feasible versus when dimensionality-aware strategies are preferred.\n",
    "\n",
    "- **Dataset size**: Captures the total number of observations to support dataset-size–aware recommendation logic.\n",
    "\n",
    "These thresholds centralize preprocessing heuristics, ensuring consistent and explainable recommendations across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bccabfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Configuration ----------------\n",
    "rare_threshold = 0.02\n",
    "missing_low = 0.05\n",
    "missing_medium = 0.30\n",
    "missing_high = 0.50\n",
    "small_dataset_threshold = 10_000\n",
    "dataset_size = len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b10e39",
   "metadata": {},
   "source": [
    "### Ordinal Feature Configuration\n",
    "\n",
    "This section loads the list of ordinal features from an external configuration file to guide preprocessing recommendations.\n",
    "\n",
    "Ordinal features are defined outside the script to separate domain knowledge from preprocessing logic. This allows ordinal semantics to be updated without modifying code and prevents these features from being incorrectly treated as nominal categorical variables.\n",
    "\n",
    "By identifying ordinal columns early, the script can exclude them from cardinality-based analyses and nominal encoding recommendations, ensuring that subsequent guidance respects the ordered nature of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4c125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Ordinal columns ----------------\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "ordinal_file = os.path.join(script_dir, \"ordinal_columns.txt\")\n",
    "with open(ordinal_file, \"r\") as f:\n",
    "    ordinal_columns = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94795dfc",
   "metadata": {},
   "source": [
    "### Numeric Preprocessing Workflow and Summary Generation\n",
    "\n",
    "This section performs a **comprehensive analysis of numeric features** to generate preprocessing recommendations and justifications, without actually transforming the dataset.\n",
    "\n",
    "#### 1. Numeric Feature Scope\n",
    "\n",
    "- Only numeric features are considered, with **ordinal columns excluded** to prevent misinterpretation.\n",
    "\n",
    "- Imputer objects (KNNImputer and IterativeImputer) are instantiated to support recommendation logic; they are **not applied to the dataset** for final preprocessing.\n",
    "\n",
    "#### 2. Missingness and Imputation Recommendations\n",
    "\n",
    "- For each numeric feature, missing count and missing ratio are computed.\n",
    "\n",
    "- **Decision rules**:\n",
    "\n",
    "    - **No missing values** → no imputation\n",
    "    - **Low missingness (≤5%)** → mean or median imputation depending on skew\n",
    "    - **Medium missingness (≤30%)** → KNN imputation\n",
    "    - **High missingness (>30%)** → iterative imputation\n",
    "    - **Very high missingness (>50%)** → iterative imputation + consider dropping\n",
    "\n",
    "These calculations ensure that each feature’s missingness is addressed analytically, supporting transparent recommendations.\n",
    "\n",
    "#### 3. Skew-Based Transformation\n",
    "\n",
    "- **Relative skew** is calculated as:\n",
    "\n",
    "        **relative skew = (mean − median) ÷ standard deviation**\n",
    "\n",
    "- **Decision**: If |relative skew| > 0.5, a log transformation is recommended to reduce asymmetry; otherwise, no transformation is suggested.\n",
    "\n",
    "#### 4. Outlier Detection and Capping\n",
    "\n",
    "- Outliers are identified using the **IQR method**:\n",
    "\n",
    "    - Q1 = 25th percentile, Q3 = 75th percentile, IQR = Q3 − Q1\n",
    "\n",
    "    - Lower bound = Q1 − 1.5 × IQR, Upper bound = Q3 + 1.5 × IQR\n",
    "\n",
    "- **Outlier factor** = (number of outlier rows ÷ total rows) × 100\n",
    "\n",
    "- **Decision**: If outlier factor > 1%, **capping is recommended**; otherwise, no capping.\n",
    "\n",
    "#### 5. Scaling Recommendation\n",
    "\n",
    "- Scaling is determined based on the **feature’s standard deviation**:\n",
    "\n",
    "    - Features with significant variance → StandardScaler\n",
    "\n",
    "    - Low-variance or stable-range features (e.g., AveBedrms) → optional scaling\n",
    "\n",
    "#### 6. Justification Text\n",
    "\n",
    "For each feature, a human-readable explanation is generated summarizing:\n",
    "\n",
    "- Missingness and imputation choice\n",
    "\n",
    "- Correlation with the target\n",
    "\n",
    "- Skew and recommended transformation\n",
    "\n",
    "- Outlier factor and capping recommendation\n",
    "\n",
    "- Standard deviation and scaling advice\n",
    "\n",
    "These justifications are combined into a single string per feature and stored alongside numeric metrics.\n",
    "\n",
    "#### 7. Summary Table and Export\n",
    "\n",
    "- All feature metrics, decisions, and justifications are consolidated into a **Pandas DataFrame**.\n",
    "\n",
    "- The DataFrame is exported as numeric_EDA_summary.csv for documentation and review, providing a **complete, reproducible record of numeric preprocessing recommendations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ecb5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Numeric preprocessing ----------------\n",
    "numeric_cols = [c for c in X_train.select_dtypes(include=[np.number]).columns if c not in ordinal_columns]\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "iter_imputer = IterativeImputer(random_state=42)\n",
    "imputed_features = {}\n",
    "n_rows_train = len(X_train)\n",
    "fill_values_dict = {}\n",
    "numeric_results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    series = X_train[[col]]\n",
    "    non_null = series.dropna()\n",
    "    missing_count = series[col].isna().sum()\n",
    "    missing_ratio = missing_count / n_rows_train\n",
    "\n",
    "    # Decide imputation\n",
    "    consider_drop_flag = False\n",
    "    if missing_count == 0:\n",
    "        temp_series = series[col].copy()\n",
    "        imputer_type = \"None\"\n",
    "    elif missing_ratio <= missing_low:\n",
    "        mean, median = non_null[col].mean(), non_null[col].median()\n",
    "        fill_value = median if median != 0 and abs(mean - median)/abs(median) >= 0.1 else mean\n",
    "        temp_series = series[col].fillna(fill_value)\n",
    "        fill_values_dict[col] = fill_value\n",
    "        imputer_type = \"Median\" if fill_value==median else \"Mean\"\n",
    "    elif missing_ratio <= missing_medium:\n",
    "        temp_series = pd.Series(knn_imputer.fit_transform(series), index=series.index, name=col)\n",
    "        imputer_type = \"KNN\"\n",
    "    elif missing_ratio <= missing_high:\n",
    "        temp_series = pd.Series(iter_imputer.fit_transform(series), index=series.index, name=col)\n",
    "        imputer_type = \"Iterative\"\n",
    "    else:\n",
    "        temp_series = pd.Series(iter_imputer.fit_transform(series), index=series.index, name=col)\n",
    "        imputer_type = \"Iterative\"\n",
    "        consider_drop_flag = True\n",
    "\n",
    "    imputed_features[col] = temp_series\n",
    "\n",
    "X_train_imputed = pd.DataFrame(imputed_features)\n",
    "feature_target_corr = X_train_imputed.corrwith(y_train)\n",
    "\n",
    "# ----- Full numeric summary with outliers, skew-based transform, scaling, justification -----\n",
    "for col in numeric_cols:\n",
    "    temp_series = X_train_imputed[col]\n",
    "    std = temp_series.std()\n",
    "    rel_skew = 0 if std == 0 else (temp_series.mean()-temp_series.median())/std\n",
    "    corr_with_target = feature_target_corr[col]\n",
    "    missing_count = X_train[col].isna().sum()\n",
    "    missing_ratio = missing_count / n_rows_train\n",
    "    drop_reasons = [\"excessive missingness\"] if missing_ratio>0.50 else []\n",
    "    recommended_drop = \", \".join(drop_reasons) if drop_reasons else \"\"\n",
    "    drop_flag = \"Yes\" if recommended_drop else \"\"\n",
    "\n",
    "    # --------- Skew-based transform ----------\n",
    "    if abs(rel_skew) > 0.5:  # you can tweak this threshold\n",
    "        transform = \"log\"\n",
    "    else:\n",
    "        transform = \"none\"\n",
    "\n",
    "    # --------- Outlier factor & capping ----------\n",
    "    Q1, Q3 = temp_series.quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_upper = Q3 + 1.5*IQR\n",
    "    outlier_lower = Q1 - 1.5*IQR\n",
    "    outlier_count = ((temp_series < outlier_lower) | (temp_series > outlier_upper)).sum()\n",
    "    outlier_factor = round(outlier_count / n_rows_train * 100, 2)  # percentage of rows\n",
    "    cap_outliers = \"yes\" if outlier_factor > 1 else \"no\"  # can adjust threshold\n",
    "\n",
    "    # --------- Scaling recommendation ----------\n",
    "    if col in [\"AveBedrms\"]:  # example for optional scaling, customize as needed\n",
    "        scaling = \"optional\"\n",
    "    else:\n",
    "        scaling = \"StandardScaler\"\n",
    "    \n",
    "    # --------- Justification text ----------\n",
    "    justification_parts = []\n",
    "\n",
    "    if missing_count == 0:\n",
    "        justification_parts.append(f\"The feature '{col}' has no missing values, so no imputation is required.\")\n",
    "    else:\n",
    "        justification_parts.append(f\"The feature '{col}' has {missing_ratio:.2%} missing values, imputed via {imputer_type}.\")\n",
    "\n",
    "    justification_parts.append(f\"The feature '{col}' has a correlation of {round(corr_with_target,2)} with the target variable.\")\n",
    "    justification_parts.append(f\"The distribution of '{col}' shows a skew of {round(rel_skew,2)}, and \" + \n",
    "                               (f\"a '{transform}' transformation is recommended to reduce skew.\" if transform!=\"none\" else \"no transformation is necessary.\"))\n",
    "    justification_parts.append(f\"The outlier factor for '{col}' is {round(outlier_factor,2)}, which indicates \" + \n",
    "                               (\"high\" if outlier_factor>5 else \"low\") + \" outlier severity. \" +\n",
    "                               (\"Capping of outliers is recommended.\" if cap_outliers==\"yes\" else \"No capping is required.\"))\n",
    "    justification_parts.append(f\"The standard deviation of '{col}' is {round(std,2)}, which means that scaling is {scaling.lower()}.\")\n",
    "\n",
    "    justification = \" \".join(justification_parts)\n",
    "\n",
    "    numeric_results.append({\n",
    "        \"Feature\": col,\n",
    "        \"MissingRatio\": round(missing_ratio,2),\n",
    "        \"Imputer\": imputer_type,\n",
    "        \"CorrWithTarget\": round(corr_with_target,2),\n",
    "        \"Skew\": round(rel_skew,2),\n",
    "        \"Transform\": transform,\n",
    "        \"OutlierFactor\": round(outlier_factor,2),\n",
    "        \"Cap Outliers\": cap_outliers,\n",
    "        \"Std\": round(std,2),\n",
    "        \"Scaling\": scaling,\n",
    "        \"RecommendedDrop\": recommended_drop,\n",
    "        \"DropFlag\": drop_flag,\n",
    "        \"Justification\": justification\n",
    "    })\n",
    "\n",
    "# ----- Export to CSV -----\n",
    "pd.DataFrame(numeric_results).to_csv(\"numeric_EDA_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc29cc",
   "metadata": {},
   "source": [
    "### Categorical Preprocessing Workflow and Summary Generation  \n",
    "\n",
    "This section evaluates **categorical features** to provide preprocessing recommendations, including missing value handling, encoding, rare category analysis, and drop guidance.\n",
    "\n",
    "#### 1. Categorical Feature Scope\n",
    "- All columns with `object` or `category` data types are included.  \n",
    "- If no categorical columns are detected, a **placeholder entry** is created stating: *“No categorical columns were detected in the dataset.”*  \n",
    "- This ensures the summary table is complete and prevents downstream errors.\n",
    "\n",
    "#### 2. Missing Value Assessment and Imputation\n",
    "- For each categorical feature:  \n",
    "  - Missing count and missing ratio are calculated.  \n",
    "  - A correlation between missingness and the target is computed to detect **informative missing values**.  \n",
    "- **Imputation decision rules:**  \n",
    "  - No missing values → no imputation  \n",
    "  - Low missingness → impute via mode  \n",
    "  - Medium missingness → impute with mode unless missingness is informative, then mark as `\"missing\"`  \n",
    "  - High missingness → mark as `\"missing\"`  \n",
    "  - Very high missingness → recommend **consider dropping**  \n",
    "\n",
    "- Imputation flags and drop flags are recorded for clarity.\n",
    "\n",
    "#### 3. Cardinality and Rare Category Analysis\n",
    "- **Cardinality classification:**  \n",
    "  - Low (≤10 unique values)  \n",
    "  - Medium (11–20 unique values)  \n",
    "  - High (>20 unique values)  \n",
    "- Rare categories are identified as those below a frequency threshold (`rare_threshold`).  \n",
    "- The proportion of rare categories is categorized as **few, moderate, many, or dominantly rare** to guide feature treatment and encoding decisions.\n",
    "\n",
    "#### 4. Encoding and High-Cardinality Decisions\n",
    "- Encoding recommendations depend on feature type and cardinality:  \n",
    "  - Ordinal → **ordinal encoding**  \n",
    "  - Binary → no encoding required  \n",
    "  - Low cardinality → **one-hot encoding**  \n",
    "  - Medium cardinality → one-hot if dataset is small; otherwise, **target encoding**  \n",
    "  - High cardinality (>20):  \n",
    "    - If correlation with target is low → consider dropping  \n",
    "    - Otherwise → **target encoding**  \n",
    "\n",
    "- Drop decisions due to high cardinality are explicitly flagged when applicable.\n",
    "\n",
    "#### 5. Justification Text\n",
    "- A human-readable justification is constructed for each feature summarizing:  \n",
    "  - Missingness level and imputation strategy  \n",
    "  - Informative missingness if present  \n",
    "  - Cardinality classification  \n",
    "  - Rare category prevalence  \n",
    "  - Ordinal vs nominal nature  \n",
    "  - Encoding recommendation or drop consideration  \n",
    "\n",
    "This ensures **every recommendation is transparent** and tied to specific feature properties.\n",
    "\n",
    "#### 6. Summary Table and Export\n",
    "- All metrics, decisions, and justifications are consolidated into a **Pandas DataFrame** (`categorical_EDA_summary`).  \n",
    "- The summary is exported as a CSV file for documentation, reporting, and review.  \n",
    "\n",
    "This provides a **complete, reproducible record of categorical preprocessing recommendations** without altering the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc8b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Categorical preprocessing ----------------\n",
    "cat_cols = X_train.select_dtypes(include=['object','category'])\n",
    "categorical_results = []\n",
    "\n",
    "if cat_cols.empty:\n",
    "    categorical_results.append({\n",
    "        \"Col\": \"—\",\n",
    "        \"Missing %\": \"\",\n",
    "        \"Impute?\": \"\",\n",
    "        \"Imputation Type\": \"\",\n",
    "        \"Drop Due to Missing?\": \"\",\n",
    "        \"Drop Due to High Cardinality?\": \"\",\n",
    "        \"Cardinality\": \"\",\n",
    "        \"Rare Category Level\": \"\",\n",
    "        \"Encoding\": \"\",\n",
    "        \"Justification\": \"No categorical columns were detected in the dataset.\"\n",
    "    })\n",
    "\n",
    "for col in cat_cols.columns:\n",
    "    series = cat_cols[col]\n",
    "    n_rows = len(series)\n",
    "    missing_count = series.isna().sum()\n",
    "    missing_ratio = missing_count / n_rows\n",
    "    corr_with_missing = series.isna().astype(int).corr(y_train) if missing_count > 0 else 0\n",
    "    informative_missing = abs(corr_with_missing) >= 0.1\n",
    "\n",
    "    # ---------- Determine imputation type ----------\n",
    "    if missing_count == 0:\n",
    "        impute_type = \"none\"\n",
    "    elif missing_ratio <= missing_low:\n",
    "        impute_type = \"mode\"\n",
    "    elif missing_low < missing_ratio <= missing_medium:\n",
    "        impute_type = \"missing\" if informative_missing else \"mode\"\n",
    "    elif missing_medium < missing_ratio <= missing_high:\n",
    "        impute_type = \"missing\"\n",
    "    else:\n",
    "        impute_type = \"consider drop\"\n",
    "\n",
    "    # ---------- Temporary imputed view ----------\n",
    "    if impute_type == \"mode\":\n",
    "        temp_series = series.fillna(series.mode(dropna=True)[0])\n",
    "    else:\n",
    "        temp_series = series.fillna(\"Missing\")\n",
    "\n",
    "    # ---------- Flags ----------\n",
    "    drop_due_to_missing = \"Yes\" if impute_type == \"consider drop\" else \"No\"\n",
    "    impute_flag = \"n/a\" if impute_type == \"consider drop\" else (\"yes\" if missing_count > 0 else \"no\")\n",
    "\n",
    "    # ---------- Cardinality ----------\n",
    "    n_unique = temp_series.nunique(dropna=True)\n",
    "    if n_unique <= 10:\n",
    "        cardinality = f\"low ({n_unique})\"\n",
    "    elif 11 <= n_unique <= 20:\n",
    "        cardinality = f\"medium ({n_unique})\"\n",
    "    else:\n",
    "        cardinality = f\"high ({n_unique})\"\n",
    "\n",
    "    # ---------- Rare category analysis ----------\n",
    "    value_counts = temp_series.value_counts(normalize=True)\n",
    "    rare_categories = value_counts[value_counts < rare_threshold].index\n",
    "    percent_rare = len(rare_categories) / n_unique if n_unique > 0 else 0\n",
    "    rare_label = (\"few\" if percent_rare <= 0.2 else \n",
    "                  \"moderate\" if percent_rare <= 0.3 else \n",
    "                  \"many\" if percent_rare <= 0.5 else \n",
    "                  \"dominantly rare\")\n",
    "\n",
    "    # ---------- Encoding / High cardinality drop decision ----------\n",
    "    is_ordinal = col in ordinal_columns\n",
    "\n",
    "    if is_ordinal:\n",
    "        encoding = \"ordinal\"\n",
    "        drop_due_to_high_cardinality = \"No\"\n",
    "    elif n_unique == 2:\n",
    "        encoding = \"binary / no encoding\"\n",
    "        drop_due_to_high_cardinality = \"No\"\n",
    "    elif n_unique <= 10:\n",
    "        encoding = \"one-hot\"\n",
    "        drop_due_to_high_cardinality = \"No\"\n",
    "    elif 11 <= n_unique <= 20:\n",
    "        if dataset_size < small_dataset_threshold:\n",
    "            encoding = \"one-hot\"\n",
    "        else:\n",
    "            encoding = \"target encoding\"\n",
    "        drop_due_to_high_cardinality = \"No\"\n",
    "    else:  # n_unique > 20\n",
    "        # Compute proxy correlation with target\n",
    "        encoded_series = pd.factorize(temp_series)[0]\n",
    "        corr_with_target = pd.Series(encoded_series).corr(y_train)\n",
    "        if abs(corr_with_target) < 0.05:\n",
    "            encoding = \"consider drop\"\n",
    "            drop_due_to_high_cardinality = \"Yes\"\n",
    "        else:\n",
    "            encoding = \"target encoding\"\n",
    "            drop_due_to_high_cardinality = \"No\"\n",
    "\n",
    "    # ---------- Justification ----------\n",
    "    justification_parts = []\n",
    "\n",
    "    if missing_count == 0:\n",
    "        justification_parts.append(\"No missing values detected\")\n",
    "    elif impute_type == \"consider drop\":\n",
    "        justification_parts.append(f\"{missing_ratio:.0%} of values are missing; consider dropping;\")\n",
    "    else:\n",
    "        justification_parts.append(f\"{missing_ratio:.0%} of values are missing\")\n",
    "        if informative_missing:\n",
    "            justification_parts.append(\"missing values are related to the target\")\n",
    "        justification_parts.append(f\"imputation via '{impute_type}' is appropriate\")\n",
    "\n",
    "    justification_parts.append(f\"the column has {cardinality} cardinality\")\n",
    "\n",
    "    if rare_categories.any():\n",
    "        justification_parts.append(f\"{rare_label} rare categories were identified\")\n",
    "\n",
    "    if is_ordinal:\n",
    "        justification_parts.append(\"categories have a natural order\")\n",
    "    else:\n",
    "        justification_parts.append(\"categories are nominal\")\n",
    "\n",
    "    # High cardinality justification\n",
    "    if n_unique > 20:\n",
    "        if drop_due_to_high_cardinality == \"Yes\":\n",
    "            justification_parts.append(f\"high cardinality and low correlation with target; consider dropping;\")\n",
    "        else:\n",
    "            justification_parts.append(\"high cardinality; target encoding is recommended;\")\n",
    "\n",
    "    if encoding != \"consider drop\":\n",
    "        justification_parts.append(f\"{encoding} is recommended to balance model stability and complexity\")\n",
    "\n",
    "    justification = \"; \".join(justification_parts)\n",
    "\n",
    "    # ---------- Store results ----------\n",
    "    categorical_results.append({\n",
    "        \"Col\": col,\n",
    "        \"Missing %\": round(missing_ratio, 2),\n",
    "        \"Impute?\": impute_flag,\n",
    "        \"Imputation Type\": impute_type,\n",
    "        \"Drop Due to Missing?\": drop_due_to_missing,\n",
    "        \"Drop Due to High Cardinality?\": drop_due_to_high_cardinality,\n",
    "        \"Cardinality\": cardinality,\n",
    "        \"Rare Category Level\": rare_label,\n",
    "        \"Encoding\": encoding,\n",
    "        \"Justification\": justification\n",
    "    })\n",
    "\n",
    "# ---------- Export to CSV ----------\n",
    "pd.DataFrame(categorical_results).to_csv(\"categorical_EDA_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7193f0",
   "metadata": {},
   "source": [
    "### Multicollinearity Detection and Feature Drop Recommendations  \n",
    "\n",
    "This section identifies **multicollinearity risks** across numeric and categorical features and produces explicit, target-aware drop recommendations. All findings are consolidated into a CSV audit file.\n",
    "\n",
    "#### 1. Correlation Threshold Definition\n",
    "- A **high-correlation threshold** is defined as:\n",
    "\n",
    "  |corr(Xᵢ, Xⱼ)| ≥ 0.80  \n",
    "\n",
    "- Any feature pair exceeding this threshold is considered a multicollinearity risk and evaluated for potential feature removal.\n",
    "\n",
    "#### 2. Categorical Imputation & Drop Context\n",
    "- Lookup tables are created from prior categorical preprocessing results:\n",
    "  - **Imputation Type** (e.g., mode, missing, consider drop)\n",
    "  - **Drop Due to Missing?**\n",
    "- These lookups allow multicollinearity decisions to incorporate **data quality context**, not just correlation strength.\n",
    "\n",
    "#### 3. One-Hot Encoding of Categorical Features\n",
    "- All categorical columns are one-hot encoded.\n",
    "- No dummy is dropped at this stage (`drop_first=False`) so that:\n",
    "  - Dummy-variable traps can be explicitly detected\n",
    "  - Root-column–level decisions remain traceable\n",
    "\n",
    "- A mapping is created from each one-hot column back to its **root categorical feature** to preserve interpretability.\n",
    "\n",
    "#### 4. Feature Space Construction\n",
    "- The analysis operates on a **combined feature matrix**:\n",
    "  - Imputed numeric features\n",
    "  - One-hot encoded categorical features\n",
    "\n",
    "This ensures that all correlation checks are performed in the **actual modeling feature space**.\n",
    "\n",
    "#### 5. Missing Value Warnings\n",
    "\n",
    "##### Categorical\n",
    "- Any categorical feature previously marked as:\n",
    "  - `consider drop`, or\n",
    "  - `Drop Due to Missing = Yes`\n",
    "- Generates a **Missing Value Warning** entry.\n",
    "- These warnings propagate to all derived one-hot features.\n",
    "\n",
    "##### Numeric\n",
    "- Numeric features flagged earlier for excessive missingness (>50%) are also recorded.\n",
    "- These warnings explicitly note that values were temporarily imputed and should be reconsidered.\n",
    "\n",
    "#### 6. Categorical–Categorical Correlations\n",
    "- Pairwise absolute correlations are computed between all one-hot encoded categorical features.\n",
    "- When correlation exceeds the threshold:\n",
    "  - Each feature’s correlation with the target is computed.\n",
    "  - The feature with **lower absolute target correlation** is recommended for removal.\n",
    "- If either root feature has a missing-value warning:\n",
    "  - That context is explicitly included in the recommendation.\n",
    "\n",
    "**Decision rule:**  \n",
    "Drop argmin(|corr(feature, target)|)\n",
    "\n",
    "#### 7. Numeric–Numeric Correlations\n",
    "- Pairwise correlations are computed across all numeric features.\n",
    "- For any pair exceeding the threshold:\n",
    "  - Both features’ correlations with the target are evaluated.\n",
    "  - The feature less predictive of the target is recommended for removal.\n",
    "\n",
    "This preserves **predictive signal while reducing redundancy**.\n",
    "\n",
    "#### 8. Categorical–Numeric Correlations\n",
    "- Each one-hot encoded categorical feature is checked against each numeric feature.\n",
    "- If correlation exceeds the threshold:\n",
    "  - No automatic drop is applied due to mixed feature types.\n",
    "  - If the categorical root feature has a missing-value warning, a drop suggestion is allowed.\n",
    "  - Otherwise, the issue is documented without forcing removal.\n",
    "\n",
    "This avoids unsafe drops caused by encoding artifacts.\n",
    "\n",
    "#### 9. Dummy-Variable Trap Detection\n",
    "- For each categorical root feature:\n",
    "  - All associated one-hot columns are checked for perfect correlation:\n",
    "\n",
    "  |corr(dummyᵢ, dummyⱼ)| ≈ 1.0  \n",
    "\n",
    "- When detected:\n",
    "  - The dummy with lower target correlation is recommended for removal.\n",
    "  - If the root feature had a missing-value warning, this context is included.\n",
    "\n",
    "This explicitly resolves **linear dependence introduced by full one-hot encoding**.\n",
    "\n",
    "#### 10. Output Artifact\n",
    "- All findings are written to a single CSV file (multicollinearity_EDA_Summary):\n",
    "  - Section (context)\n",
    "  - Feature pair\n",
    "  - Correlation between features\n",
    "  - Correlation with target\n",
    "  - Drop recommendation and rationale\n",
    "\n",
    "The result is a **fully auditable, target-aware multicollinearity report** suitable for portfolio-grade documentation and downstream modeling decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5c75166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Multicollinearity Check ----------------\n",
    "\n",
    "# Threshold for high correlation\n",
    "corr_threshold = 0.8\n",
    "recommendation_file_csv = \"multicollinearity_EDA_Summary.csv\"\n",
    "\n",
    "\n",
    "# Prepare categorical imputation lookup\n",
    "# -----------------------------\n",
    "cat_impute_lookup = {row['Col']: row['Imputation Type'] for row in categorical_results}\n",
    "cat_drop_lookup = {row['Col']: row['Drop Due to Missing?'] for row in categorical_results}\n",
    "\n",
    "# One-hot encode categorical features\n",
    "# -----------------------------\n",
    "category_ohe = pd.get_dummies(X_train.select_dtypes(include=['object','category']), drop_first=False)\n",
    "category_ohe_copy = category_ohe.copy()\n",
    "\n",
    "# Map one-hot columns to root column\n",
    "ohe_to_root = {ohe_col: root_col\n",
    "               for root_col in X_train.select_dtypes(include=['object','category']).columns\n",
    "               for ohe_col in category_ohe_copy.columns\n",
    "               if ohe_col.startswith(root_col + \"_\")}\n",
    "\n",
    "# Combine numeric + categorical features\n",
    "# -----------------------------\n",
    "X_train_combined = pd.concat([X_train_imputed, category_ohe_copy], axis=1)\n",
    "numeric_cols = X_train_imputed.columns\n",
    "categorical_cols = category_ohe_copy.columns\n",
    "\n",
    "# Prepare CSV rows\n",
    "# -----------------------------\n",
    "csv_rows = []\n",
    "\n",
    "if cat_cols.empty:\n",
    "    csv_rows.append({\n",
    "        \"Section\": \"Info\",\n",
    "        \"Feature 1\": \"\",\n",
    "        \"Feature 2\": \"\",\n",
    "        \"Feature 1 Corr with Target\": \"\",\n",
    "        \"Feature 2 Corr with Target\": \"\",\n",
    "        \"Correlation Between Features\": \"\",\n",
    "        \"Drop Suggestion / Notes\": \"No categorical columns were detected in the dataset; categorical multicollinearity checks were skipped.\"\n",
    "    })\n",
    "\n",
    "# ---- Missing Value Warnings: Categorical ----\n",
    "for row in categorical_results:\n",
    "    if row['Imputation Type'] == \"consider drop\" or row['Drop Due to Missing?'] == \"Yes\":\n",
    "        csv_rows.append({\n",
    "            \"Section\": \"Missing Value Warnings\",\n",
    "            \"Feature 1\": row['Col'],\n",
    "            \"Feature 2\": \"\",\n",
    "            \"Feature 1 Corr with Target\": \"\",\n",
    "            \"Feature 2 Corr with Target\": \"\",\n",
    "            \"Correlation Between Features\": \"\",\n",
    "            \"Drop Suggestion / Notes\": f\"Feature(s) associated with root column '{row['Col']}' have a Missing Value Warning; temporarily filled 'Missing'; consider dropping\"\n",
    "        })\n",
    "\n",
    "# ---- Missing Value Warnings: Numeric ----\n",
    "for row in numeric_results:\n",
    "    if row['RecommendedDrop']:\n",
    "        csv_rows.append({\n",
    "            \"Section\": \"Missing Value Warnings\",\n",
    "            \"Feature 1\": row['Feature'],\n",
    "            \"Feature 2\": \"\",\n",
    "            \"Feature 1 Corr with Target\": \"\",\n",
    "            \"Feature 2 Corr with Target\": \"\",\n",
    "            \"Correlation Between Features\": \"\",\n",
    "            \"Drop Suggestion / Notes\": f\"Feature '{row['Feature']}' has excessive missing values; temporarily imputed; consider dropping\"\n",
    "        })\n",
    "\n",
    "#Categorical-Categorical correlations\n",
    "# -----------------------------\n",
    "cat_corr = X_train_combined[categorical_cols].corr().abs()\n",
    "for i in range(len(categorical_cols)):\n",
    "    for j in range(i+1, len(categorical_cols)):\n",
    "        if cat_corr.iloc[i, j] > corr_threshold:\n",
    "            col1, col2 = categorical_cols[i], categorical_cols[j]\n",
    "            corr1 = X_train_combined[col1].corr(y_train)\n",
    "            corr2 = X_train_combined[col2].corr(y_train)\n",
    "            root1, root2 = ohe_to_root.get(col1,\"\"), ohe_to_root.get(col2,\"\")\n",
    "            \n",
    "            # Determine notes\n",
    "            if cat_impute_lookup.get(root1) == \"consider drop\" and cat_impute_lookup.get(root2) == \"consider drop\":\n",
    "                notes = f\"Features associated with root columns '{root1}', '{root2}' have Missing Value Warnings; consider dropping. Suggest drop: {col1 if abs(corr1)<abs(corr2) else col2} (lower correlation with target)\"\n",
    "            elif cat_impute_lookup.get(root1) == \"consider drop\":\n",
    "                notes = f\"Feature associated with root column '{root1}' has Missing Value Warning; consider dropping. Suggest drop: {col1}\"\n",
    "            elif cat_impute_lookup.get(root2) == \"consider drop\":\n",
    "                notes = f\"Feature associated with root column '{root2}' has Missing Value Warning; consider dropping. Suggest drop: {col2}\"\n",
    "            else:\n",
    "                notes = f\"Suggest drop: {col1 if abs(corr1)<abs(corr2) else col2} (lower correlation with target)\"\n",
    "\n",
    "            csv_rows.append({\n",
    "                \"Section\": \"Categorical-Categorical Correlations\",\n",
    "                \"Feature 1\": col1,\n",
    "                \"Feature 2\": col2,\n",
    "                \"Feature 1 Corr with Target\": round(corr1,2),\n",
    "                \"Feature 2 Corr with Target\": round(corr2,2),\n",
    "                \"Correlation Between Features\": round(cat_corr.iloc[i,j],2),\n",
    "                \"Drop Suggestion / Notes\": notes\n",
    "            })\n",
    "\n",
    "# Numeric-Numeric correlations\n",
    "# -----------------------------\n",
    "num_corr = X_train_combined[numeric_cols].corr().abs()\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        if num_corr.iloc[i,j] > corr_threshold:\n",
    "            col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "            corr1 = X_train_combined[col1].corr(y_train)\n",
    "            corr2 = X_train_combined[col2].corr(y_train)\n",
    "            drop_col = col1 if abs(corr1)<abs(corr2) else col2\n",
    "            csv_rows.append({\n",
    "                \"Section\": \"Numeric-Numeric Correlations\",\n",
    "                \"Feature 1\": col1,\n",
    "                \"Feature 2\": col2,\n",
    "                \"Feature 1 Corr with Target\": round(corr1,2),\n",
    "                \"Feature 2 Corr with Target\": round(corr2,2),\n",
    "                \"Correlation Between Features\": round(num_corr.iloc[i,j],2),\n",
    "                \"Drop Suggestion / Notes\": f\"Suggest drop: {drop_col} (lower correlation with target)\"\n",
    "            })\n",
    "\n",
    "# Categorical-Numeric correlations\n",
    "# -----------------------------\n",
    "for cat_col in categorical_cols:\n",
    "    for num_col in numeric_cols:\n",
    "        corr_value = X_train_combined[cat_col].corr(X_train_combined[num_col])\n",
    "        if abs(corr_value) > corr_threshold:\n",
    "            root = ohe_to_root.get(cat_col,\"\")\n",
    "            if cat_impute_lookup.get(root)==\"consider drop\":\n",
    "                notes = f\"Feature associated with root column '{root}' has a Missing Value Warning; consider dropping. Suggest drop: {cat_col} (lower correlation with target)\"\n",
    "            else:\n",
    "                notes = \"No drop suggested (mixed feature types, drop recommendation not applied)\"\n",
    "            csv_rows.append({\n",
    "                \"Section\":\"Categorical-Numeric Correlations\",\n",
    "                \"Feature 1\": cat_col,\n",
    "                \"Feature 2\": num_col,\n",
    "                \"Feature 1 Corr with Target\": round(X_train_combined[cat_col].corr(y_train),2),\n",
    "                \"Feature 2 Corr with Target\": round(X_train_combined[num_col].corr(y_train),2),\n",
    "                \"Correlation Between Features\": round(corr_value,2),\n",
    "                \"Drop Suggestion / Notes\": notes\n",
    "            })\n",
    "\n",
    "# Dummy-variable trap\n",
    "# -----------------------------\n",
    "for root_col in X_train.select_dtypes(include=['object','category']).columns:\n",
    "    ohe_cols = [c for c in category_ohe_copy.columns if c.startswith(root_col+\"_\")]\n",
    "    for i in range(len(ohe_cols)):\n",
    "        for j in range(i+1,len(ohe_cols)):\n",
    "            corr_value = X_train_combined[ohe_cols[i]].corr(X_train_combined[ohe_cols[j]])\n",
    "            if abs(corr_value-1.0)<1e-8:\n",
    "                corr1 = X_train_combined[ohe_cols[i]].corr(y_train)\n",
    "                corr2 = X_train_combined[ohe_cols[j]].corr(y_train)\n",
    "                drop_col = ohe_cols[i] if abs(corr1)<abs(corr2) else ohe_cols[j]\n",
    "                if cat_impute_lookup.get(root_col)==\"consider drop\":\n",
    "                    notes = f\"Original feature ({root_col}) has a Missing Value Warning; consider dropping. Suggest drop: {drop_col} (lower correlation with target)\"\n",
    "                else:\n",
    "                    notes = f\"Suggest drop: {drop_col} (lower correlation with target)\"\n",
    "                csv_rows.append({\n",
    "                    \"Section\":\"Dummy-Variable Trap\",\n",
    "                    \"Feature 1\": ohe_cols[i],\n",
    "                    \"Feature 2\": ohe_cols[j],\n",
    "                    \"Feature 1 Corr with Target\": round(corr1,2),\n",
    "                    \"Feature 2 Corr with Target\": round(corr2,2),\n",
    "                    \"Correlation Between Features\": round(corr_value,2),\n",
    "                    \"Drop Suggestion / Notes\": notes\n",
    "                })\n",
    "\n",
    "# Save recommendations to CSV\n",
    "# -----------------------------\n",
    "df_recommendations = pd.DataFrame(csv_rows)\n",
    "df_recommendations.to_csv(recommendation_file_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
